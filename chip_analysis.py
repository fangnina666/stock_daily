import pandas as pd
import os
import re
import pandas as pd
import os

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
REPORT_DIR = os.path.join(BASE_DIR, "reports")

def load_industry_map(filepath="industry_map.csv"):
    """
    è¼‰å…¥ç”¢æ¥­å°ç…§è¡¨ (CSV or Excel)
    CSV æ¬„ä½éœ€åŒ…å«ï¼šè‚¡ç¥¨ä»£è™Ÿ,è‚¡ç¥¨åç¨±,ç”¢æ¥­
    """
    if filepath.endswith(".csv"):
        df = pd.read_csv(filepath, dtype={"è‚¡ç¥¨ä»£è™Ÿ": str})
    else:
        df = pd.read_excel(filepath, dtype={"è‚¡ç¥¨ä»£è™Ÿ": str})
    return dict(zip(df["è‚¡ç¥¨ä»£è™Ÿ"], df["ç”¢æ¥­"]))

# ============================================================
# æ‰¾æœ€è¿‘å…©æ—¥çš„æª”æ¡ˆ
# ============================================================
def get_latest_two_files(folder="reports"):
    """
    å¾æŒ‡å®šè³‡æ–™å¤¾ä¸­ï¼ŒæŒ‘é¸å‡ºæœ€è¿‘å…©å€‹æ—¥æœŸçš„æª”æ¡ˆ
    æª”åæ ¼å¼éœ€åŒ…å«: åˆ¸å•†åˆ†é»è²·è³£è¶…æ˜ç´°_YYYYMMDD.xlsx
    """
    pattern = r"åˆ¸å•†åˆ†é»è²·è³£è¶…æ˜ç´°_(\d{8})\.xlsx"
    files = []

    for f in os.listdir(folder):
        match = re.match(pattern, f)
        if match:
            date_str = match.group(1)
            files.append((date_str, os.path.join(folder, f)))

    if not files or len(files) < 2:
        raise ValueError("è³‡æ–™å¤¾å…§ä¸è¶³å…©å€‹ç¬¦åˆæ ¼å¼çš„æª”æ¡ˆ")

    # ä¾æ—¥æœŸæ’åºï¼Œå–æœ€å¾Œå…©å€‹
    files_sorted = sorted(files, key=lambda x: x[0])
    latest_two = files_sorted[-2:]

    return latest_two  # [(æ—¥æœŸå­—ä¸², è·¯å¾‘), ...]

# ============================================================
# ä¸»åˆ†ææµç¨‹
# ============================================================
def df_to_markdown_table(df, day1_label, day2_label):
    table = "| è‚¡ç¥¨ä»£è™Ÿ | è‚¡ç¥¨åç¨± | åˆ¸å•†åç¨± | {} æ·¨è²·è¶… | {} æ·¨è²·è¶… | Î” æ·¨è²·è¶… | æ˜¯å¦é¡¯è‘—ç•°å¸¸ |\n".format(day1_label, day2_label)
    table += "|----------|----------|----------|-----------|-----------|-----------|--------------|\n"
    for _, row in df.iterrows():
        abnormal_flag = "âœ…" if row["ç•°å¸¸"] else "âš ï¸"
        table += f"| {row['è‚¡ç¥¨ä»£è™Ÿ']} | {row['è‚¡ç¥¨åç¨±']} | {row['å­åˆ¸å•†åç¨±']} | {int(row[f'æ·¨è²·è¶…_{day1_label}'])} | {int(row[f'æ·¨è²·è¶…_{day2_label}'])} | {int(row['Î”æ·¨è²·è¶…'])} | {abnormal_flag} |\n"
    return table


def analyze_two_day_chip_flow(file_day1, file_day2, industry_map=None,
                              day1_label="Day1", day2_label="Day2", 
                              top_n=3, concentration_threshold=0.6,
                              min_delta=50, min_volume=200, min_broker_volume=50,
                              output_path=None, output_format="md",
                              return_concentrated_only=False):
    """
    åˆ†æå…©æ—¥ç±Œç¢¼æµå‘ + ç•¶æ—¥é›†ä¸­åº¦ + ç”¢æ¥­æ¨™ç±¤ + å ±å‘Šè¼¸å‡º
    - return_concentrated_only=True æ™‚ï¼Œå›å‚³çš„ abnormal_df åƒ…ä¿ç•™ã€ŒDay2 é«˜åº¦é›†ä¸­(å æ¯”>=threshold)ã€çš„åˆ¸å•†åˆ—ã€‚
    """

    # === è¼‰å…¥å…©æ—¥æª”æ¡ˆ ===
    df1 = pd.read_excel(file_day1, dtype={"è‚¡ç¥¨ä»£è™Ÿ": str})
    df2 = pd.read_excel(file_day2, dtype={"è‚¡ç¥¨ä»£è™Ÿ": str})

    df1 = df1.drop_duplicates(subset=["è³‡æ–™æ—¥æœŸ","è‚¡ç¥¨ä»£è™Ÿ", "è‚¡ç¥¨åç¨±", "å­åˆ¸å•†åç¨±", "è²·å…¥å¼µæ•¸", "è³£å‡ºå¼µæ•¸"])
    df2 = df2.drop_duplicates(subset=["è³‡æ–™æ—¥æœŸ","è‚¡ç¥¨ä»£è™Ÿ", "è‚¡ç¥¨åç¨±", "å­åˆ¸å•†åç¨±", "è²·å…¥å¼µæ•¸", "è³£å‡ºå¼µæ•¸"])

    # è¨ˆç®—æ·¨è²·è¶…
    df1["æ·¨è²·è¶…"] = df1["è²·å…¥å¼µæ•¸"] - df1["è³£å‡ºå¼µæ•¸"]
    df2["æ·¨è²·è¶…"] = df2["è²·å…¥å¼µæ•¸"] - df2["è³£å‡ºå¼µæ•¸"]

    # åˆä½µå…©æ—¥
    merged = pd.merge(
        df1[["è‚¡ç¥¨ä»£è™Ÿ", "è‚¡ç¥¨åç¨±", "å­åˆ¸å•†åç¨±", "æ·¨è²·è¶…"]].rename(columns={"æ·¨è²·è¶…": f"æ·¨è²·è¶…_{day1_label}"}),
        df2[["è‚¡ç¥¨ä»£è™Ÿ", "è‚¡ç¥¨åç¨±", "å­åˆ¸å•†åç¨±", "æ·¨è²·è¶…"]].rename(columns={"æ·¨è²·è¶…": f"æ·¨è²·è¶…_{day2_label}"}),
        on=["è‚¡ç¥¨ä»£è™Ÿ", "è‚¡ç¥¨åç¨±", "å­åˆ¸å•†åç¨±"],
        how="outer"
    ).fillna(0)

    # Î”æ·¨è²·è¶…
    merged["Î”æ·¨è²·è¶…"] = merged[f"æ·¨è²·è¶…_{day2_label}"] - merged[f"æ·¨è²·è¶…_{day1_label}"]

    # è¨ˆç®— Day2 å¹³å‡ & æ¨™æº–å·®
    stats_day2 = merged.groupby("è‚¡ç¥¨ä»£è™Ÿ")[f"æ·¨è²·è¶…_{day2_label}"].agg(["mean", "std"]).reset_index()
    merged = merged.merge(stats_day2, on="è‚¡ç¥¨ä»£è™Ÿ", how="left")

    # âš ï¸ åš´æ ¼ç‰ˆç•°å¸¸æ¢ä»¶
    merged["ç•°å¸¸"] = (
        (merged["Î”æ·¨è²·è¶…"] > min_delta) &
        (merged[f"æ·¨è²·è¶…_{day2_label}"] > min_volume) &
        (merged[f"æ·¨è²·è¶…_{day2_label}"] > merged["mean"] + 2 * merged["std"])
    )

    # éæ¿¾ ETF (00 é–‹é ­)
    abnormal_df = merged[(merged["ç•°å¸¸"]) & (~merged["è‚¡ç¥¨ä»£è™Ÿ"].astype(str).str.startswith("00"))]

    # === Day2 ç±Œç¢¼çµæ§‹ï¼ˆæ”¹ï¼šç”¨ã€Œæ­£å‘è²·æ–¹å æ¯”ã€é¿å…æ­£è² äº’æŠµé€ æˆå‡é«˜å æ¯”ï¼‰===
    agg_day2 = df2.groupby(["è‚¡ç¥¨ä»£è™Ÿ", "è‚¡ç¥¨åç¨±", "å­åˆ¸å•†åç¨±"])["æ·¨è²·è¶…"].sum().reset_index()
    
    # éæ¿¾æ‰ ETF & å°æ–¼ min_broker_volume çš„åˆ¸å•†
    agg_day2 = agg_day2[
        (~agg_day2["è‚¡ç¥¨ä»£è™Ÿ"].astype(str).str.startswith("00")) &
        (agg_day2["æ·¨è²·è¶…"].abs() >= min_broker_volume)
    ].copy()
    
    # âœ… åªçœ‹ã€Œæ­£å‘æ·¨è²·è¶…ã€åšå æ¯”ï¼ˆæŠŠè³£è¶…è¦–ç‚º 0ï¼Œé¿å…ç¸½æ·¨è²·è¶…=å°æ•¸å­—è¢«æ”¾å¤§ï¼‰
    agg_day2["æ­£å‘æ·¨è²·è¶…"] = agg_day2["æ·¨è²·è¶…"].clip(lower=0)
    
    # åˆ†æ¯ï¼šè©²è‚¡ç¥¨ Day2 æ‰€æœ‰åˆ¸å•†ã€Œæ­£å‘æ·¨è²·è¶…ã€åŠ ç¸½ï¼ˆè²·æ–¹ç¸½é‡ï¼‰
    stock_buy_total = (
        agg_day2.groupby("è‚¡ç¥¨ä»£è™Ÿ")["æ­£å‘æ·¨è²·è¶…"]
                .sum()
                .reset_index()
                .rename(columns={"æ­£å‘æ·¨è²·è¶…": "è²·æ–¹ç¸½é‡"})
    )
    
    merged_day2 = agg_day2.merge(stock_buy_total, on="è‚¡ç¥¨ä»£è™Ÿ", how="left")
    
    # âœ… éæ¿¾ï¼šè²·æ–¹ç¸½é‡å¤ªå°çš„è‚¡ç¥¨ä¸çœ‹ï¼ˆç”¨ min_volume ç•¶é–€æª»ï¼‰
    merged_day2 = merged_day2[merged_day2["è²·æ–¹ç¸½é‡"] >= min_volume].copy()
    
    # å æ¯”ï¼šè©²åˆ¸å•†çš„æ­£å‘æ·¨è²·è¶… / è²·æ–¹ç¸½é‡
    merged_day2["å æ¯”"] = merged_day2["æ­£å‘æ·¨è²·è¶…"] / merged_day2["è²·æ–¹ç¸½é‡"]
    
    # top_n ä¸»åŠ›åˆ¸å•†ï¼šç”¨ã€Œæ­£å‘æ·¨è²·è¶…ã€æ’åºæ›´ç›´è§€
    flow_df = (
        merged_day2.sort_values(["è‚¡ç¥¨ä»£è™Ÿ", "æ­£å‘æ·¨è²·è¶…"], ascending=[True, False])
                  .groupby("è‚¡ç¥¨ä»£è™Ÿ")
                  .head(top_n)
    )

    # === æ¿ƒç¸®ï¼šåªä¿ç•™ Day2 é«˜åº¦é›†ä¸­çš„åˆ¸å•†ï¼ˆg2 çš„è¯é›†ï¼‰===
    g2_all = flow_df[flow_df["å æ¯”"] >= concentration_threshold][
        ["è‚¡ç¥¨ä»£è™Ÿ","è‚¡ç¥¨åç¨±","å­åˆ¸å•†åç¨±"]
    ].drop_duplicates()

    abnormal_df_concentrated = abnormal_df.merge(
        g2_all, on=["è‚¡ç¥¨ä»£è™Ÿ","è‚¡ç¥¨åç¨±","å­åˆ¸å•†åç¨±"], how="inner"
    )

    # === ç”Ÿæˆ Markdown å ±å‘Š ===
    report_lines = [f"# {day2_label} ç•°å¸¸ç±Œç¢¼åˆ†æå ±å‘Š", ""]
    industry_summary = []

    # æ³¨æ„ï¼šä»¥ä¸‹å ±å‘Šæ®µè½ä¸­çš„ g2 æ”¹ç‚ºä½¿ç”¨åƒæ•¸ concentration_threshold
    # ä¸¦ä¸”è‹¥ return_concentrated_only=Trueï¼Œå¯åªä»¥ abnormal_df_concentrated ä½œç‚ºè¿­ä»£ä¾†æº
    source_abn = abnormal_df_concentrated if return_concentrated_only else abnormal_df

    for stock, g in source_abn.groupby("è‚¡ç¥¨ä»£è™Ÿ"):
        stock_name = g["è‚¡ç¥¨åç¨±"].iloc[0]
        industry = industry_map.get(str(stock), "æœªåˆ†é¡") if industry_map else "æœªåˆ†é¡"
        industry_summary.append(industry)

        report_lines.append(f"## {stock} {stock_name} ({industry})")

        for _, row in g.sort_values("Î”æ·¨è²·è¶…", ascending=False).iterrows():
            report_lines.append(
                f"- {row['å­åˆ¸å•†åç¨±']}ï¼š{day1_label} æ·¨è²·è¶… {int(row[f'æ·¨è²·è¶…_{day1_label}'])} å¼µ â†’ "
                f"{day2_label} æ·¨è²·è¶… {int(row[f'æ·¨è²·è¶…_{day2_label}'])} å¼µï¼ŒÎ” {int(row['Î”æ·¨è²·è¶…'])} å¼µ"
            )

        report_lines.append(f"### ç•¶æ—¥ç±Œç¢¼ä¸»åŠ› (åƒ…é¡¯ç¤ºå æ¯” â‰¥ {concentration_threshold:.0%})")
        g2 = flow_df[(flow_df["è‚¡ç¥¨ä»£è™Ÿ"] == stock) & (flow_df["å æ¯”"] >= concentration_threshold)]
        for _, row in g2.iterrows():
            report_lines.append(f"- {row['å­åˆ¸å•†åç¨±']}: {int(row['æ·¨è²·è¶…'])} å¼µï¼Œå æ¯” {row['å æ¯”']:.1%}")

        if not g2.empty:
            report_lines.append("ğŸ‘‰ ç±Œç¢¼é«˜åº¦é›†ä¸­ï¼Œé¡¯ç¤ºä¸»åŠ›åˆ¸å•†é–ç¢¼\n")
        else:
            report_lines.append("ğŸ‘‰ ç„¡å–®ä¸€åˆ¸å•†å æ¯”é”é–€æª»ï¼Œç±Œç¢¼æœªæ˜é¡¯é›†ä¸­\n")

    # === ç¸½çµç”¢æ¥­ ===
    if industry_summary:
        industry_counts = pd.Series(industry_summary).value_counts()
        summary_text = "ã€".join([f"{k} ({v} æª”)" for k, v in industry_counts.items()])
        report_lines.append("### ç¸½çµ")
        report_lines.append(f"ä»Šæ—¥ç•°å¸¸ç±Œç¢¼ä¸»è¦é›†ä¸­åœ¨ï¼š{summary_text}ã€‚")
    else:
        report_lines.append("### ç¸½çµ")
        report_lines.append("ä»Šæ—¥æœªè§€å¯Ÿåˆ°é¡¯è‘—ç•°å¸¸ç±Œç¢¼ã€‚")

    result_text = "\n".join(report_lines)

    # === è¼¸å‡ºå ±å‘Šæª”æ¡ˆ ===
    if output_path:
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(result_text)

    # ä¾åƒæ•¸æ±ºå®šå›å‚³å“ªä¸€å€‹ abnormal_df
    abnormal_out = abnormal_df_concentrated if return_concentrated_only else abnormal_df
    return result_text, abnormal_out, flow_df

# ============================================================
# ç”¢ç”Ÿ LLM Prompt
# ============================================================
def generate_llm_prompt(day1_label, day2_label, abnormal_df):
    base_prompt = f"""
ä½ æ˜¯ä¸€åå°è‚¡ç±Œç¢¼åˆ†æåŠ©ç†ã€‚
ä»¥ä¸‹æä¾› {day1_label} èˆ‡ {day2_label} çš„åˆ¸å•†åˆ†é»é€²å‡ºæ¯”è¼ƒçµæœã€‚
è«‹ä¾ç…§ä»¥ä¸‹ä»»å‹™ï¼Œè¼¸å‡ºä¸€ä»½ã€Œç•°å¸¸ç±Œç¢¼åˆ†æå ±å‘Šã€ï¼š

ã€ä»»å‹™è¦æ±‚ã€‘
1. æ¢åˆ—å‡ºç•°å¸¸çš„è‚¡ç¥¨èˆ‡åˆ¸å•†ï¼Œéœ€åŒ…å«ï¼š
   - è‚¡ç¥¨ä»£è™Ÿã€è‚¡ç¥¨åç¨±
   - åˆ¸å•†åç¨±
   - {day1_label} æ·¨è²·è¶…ã€{day2_label} æ·¨è²·è¶…ã€Î”æ·¨è²·è¶…
   - æ˜¯å¦é¡¯è‘—ç•°å¸¸
2. åˆ†æè©²è‚¡ç¥¨åœ¨ {day2_label} çš„ç±Œç¢¼é›†ä¸­åº¦ï¼ˆåƒ…ä¾æ“šç•°å¸¸åˆ¸å•†ï¼‰ï¼Œåˆ¤æ–·æ˜¯å¦æœ‰ä¸»åŠ›é–ç¢¼ã€‚
3. éæ¿¾æ‰ 00 é–‹é ­çš„ ETF / æŒ‡æ•¸å•†å“ã€‚
4. ç¸½çµï¼ˆ3â€“5 å¥è©±ï¼‰ï¼šæŒ‡å‡ºç•°å¸¸åˆ¸å•†é›†ä¸­åœ¨å“ªäº›è‚¡ç¥¨æˆ–ç”¢æ¥­æ—ç¾¤ï¼Œä¸¦æ¨æ¸¬å¯èƒ½çš„å¸‚å ´æ„åœ–ã€‚

ã€ç•°å¸¸æ•¸æ“šã€‘
"""

    for _, row in abnormal_df.iterrows():
        base_prompt += f"""
- {row['è‚¡ç¥¨ä»£è™Ÿ']} {row['è‚¡ç¥¨åç¨±']} / {row['å­åˆ¸å•†åç¨±']}
  {day1_label}: {int(row[f'æ·¨è²·è¶…_{day1_label}'])} å¼µ
  {day2_label}: {int(row[f'æ·¨è²·è¶…_{day2_label}'])} å¼µ
  Î”: {int(row['Î”æ·¨è²·è¶…'])} å¼µ
"""
    return base_prompt

# ============================================================
# ä¸»ç¨‹å¼
# ============================================================
if __name__ == "__main__":
    # æ‰¾æœ€è¿‘å…©æ—¥æª”æ¡ˆ
    (day1_str, file_day1), (day2_str, file_day2) = get_latest_two_files(REPORT_DIR)

    print(f"åˆ†ææœ€è¿‘å…©æ—¥æª”æ¡ˆï¼š{day1_str}, {day2_str}")

    # è¼‰å…¥ç”¢æ¥­å°ç…§è¡¨
    industry_map = load_industry_map("industry_map.csv")

    # åŸ·è¡Œåˆ†æ
    report, abnormal_df, flow_df = analyze_two_day_chip_flow(
        file_day1, file_day2,
        industry_map=industry_map,
        day1_label=day1_str,
        day2_label=day2_str,
        top_n=3,
        concentration_threshold=0.6,
        min_delta=50, min_volume=200, min_broker_volume=50,
        output_path=f"reports/abnormal_report_{day2_str}.md"
    )

    print(report)
    # ç”¢ç”Ÿ prompt.txt
    
        # åƒ…å–å‰ 5 æª”ç•°å¸¸è‚¡ç¥¨
    abnormal_top = abnormal_df.groupby("è‚¡ç¥¨ä»£è™Ÿ").head(1).head(20)
    flow_top = flow_df[flow_df["è‚¡ç¥¨ä»£è™Ÿ"].isin(abnormal_top["è‚¡ç¥¨ä»£è™Ÿ"])]
    
    # ç”¢ç”Ÿç²¾ç°¡ç‰ˆ Prompt
    prompt_text = generate_llm_prompt(day1_str, day2_str, abnormal_top)
    with open(f"reports/llm_prompt_{day2_str}.txt", "w", encoding="utf-8") as f:
        f.write(prompt_text)
     
        '''
    prompt_text = generate_llm_prompt(day1_str, day2_str, abnormal_df)
    with open(f"reports/llm_prompt_{day2_str}.txt", "w", encoding="utf-8") as f:
       f.write(prompt_text)
       '''

    print("å·²ç”Ÿæˆå ±å‘Š abnormal_report.md èˆ‡ LLM Prompt.txtï¼Œå¯ç›´æ¥ä¸Ÿçµ¦ ChatGPT/Gemini")

    markdown_table = df_to_markdown_table(abnormal_df, day1_str, day2_str)
    with open(f"reports/abnormal_table_{day2_str}.md", "w", encoding="utf-8") as f:
        f.write(markdown_table)
        
    abnormal_df.to_excel(f"reports/abnormal_table_{day2_str}.xlsx", index=False)
    
    print(markdown_table)   # åœ¨ console å°å‡ºä¾†

